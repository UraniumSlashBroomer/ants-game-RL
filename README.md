# Цель проекта:
Реализовать RL среду и алгоритм PPO (Proximal Policy Optimization) без сторонних RL-библиотек.

### Задача среды:
Агент появляется на координатах спавна. Это уникальный тайл.

У него есть следующие действия:
- движение влево/вправо/вверх/вниз
- подобрать еду
- положить еду

Его цель - собрать еду с других тайлов и принести на спавн. Когда агент принесет достаточно еды для конца эпизода - эпизод завершится.

# Установка:
Клонируем репозиторий и переходим в папку с проектом:
```
git clone https://github.com/UraniumSlashBroomer/ants-game-RL.git
cd ants-game-RL
```
Создаем виртуальную среду, активируем и устанавливаем зависимости:

```
python -m venv .env
source .env/bin/activate
pip install -r requirements.txt
```

**ВАЖНО!** В requirements находится torch без поддержки GPU. Это связано с тем, что torch с поддержкой GPU занимает 7 ГБ места, а без всего 1 ГБ. В данном проекте используются маленькие модели, из-за чего вряд ли вам понадобится поддержка GPU. Тем не менее, в проекте все равно прописан device, то есть при желании можно запускать обучение и на GPU. Во время написания проекта актуальная версия CUDA - 12.6:

```
pip install torch --index-url https://download.pytorch.org/whl/cu126
```

***
### Запуск:
```
python main.py
```

# Другое:
### Стэк проекта:
- torch для реализации моделей и их обучения
- numpy для удобных вычислений и функций
- pygame для визуализации среды
- yaml для конфига (удобного изменения параметров)

### Что можно улучшить в проекте?
- Сделать визуализацию получше, например, добавить спрайты
- Сделать нормальное версионированние моделей (пока что все сохранения моделей идут в файл checkpoint.pth)
- Добавить конфиг для функции награды. Возможно немного видоизменить функцию награды
- Разделить количество эпох обучения отдельно для actor и critic

### Что можно добавить нового в проект?
- Добавить механику очков передвижения, чтобы агент учитывал их и они влияли на его траекторию передвижения
(задел на это уже есть в проекте, но механика пока что не учитывается)
- Добавить более гибкий конфиг для архитектуры моделей
- Сделать так, чтобы на карте было сразу несколько агентов и они действовали сообща (поэтому в конфиге есть num_units)
***
### Скриншоты:

#### Обучение:
<img width="1102" height="888" alt="image" src="https://github.com/user-attachments/assets/049d1358-0272-4b7b-b7c2-f4243035a508" />

***

#### Симуляция:
<img width="1249" height="871" alt="image" src="https://github.com/user-attachments/assets/69faf08f-6332-4fbf-afce-c00ab7ff52e3" />

***

#### Ручное управление для отладки:
<img width="1290" height="985" alt="image" src="https://github.com/user-attachments/assets/0ee02382-1db0-4488-bad5-4ea49ab5aebb" />

***

